\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage[margin=1.2in]{geometry}

\begin{document}

\section*{Notation Glossary:}
\begin{tabular}{ll}
\(z_j^l\) & Pre-Activation of $j^{\text{th}}$ neuron in the $l^{\text{th}}$ layer \\[0.5em]
\(a_j^l\) & (Post)-Activation of $j^{\text{th}}$ neuron in the $l^{\text{th}}$ layer \\[0.5em]
\(x_k\) & Input ($0^{\text{th}}$) layer activations \\[0.5em]
$A()$ & General element-wise activation function \\[0.5em]
\(w_{jk}^l\) & Weight matrix of $l^{\text{th}}$ layer \\[0.5em]
\(b_j^l\) & Bias vector of $l^{\text{th}}$ layer \\[0.5em]
\(g_{jk}^l\) & Jacobian between layer \(l\) and \(l-1\) \\[0.5em]
\(J_{jk}^l\) & Jacobian between input ($0^{\text{th}}$) layer and layer \(l\) \\[0.5em]
\(H_{jk}^l\) & Diagonal Hessian between input ($0^{\text{th}}$) layer and layer \(l\) \\[0.5em]
\(\odot\) & Direct product over repeated index rather than summation \\[0.5em]
\end{tabular}

\section*{Network Jacobian:}
In following derivations assume the standard feed forward relationships:
\begin{align}
a_j^l &= A(z_j^l) \\
z_j^l &= \sum_k w_{jk}^la_k^{l-1} + b_j^l
\end{align}

Note that the network's output corresponds to the \(L^{th}\) layer activations \(a_j^L\). Begin by considering the layer to layer Jacobian:
\begin{align}
\frac{\partial a_j^l}{\partial a_k^{l-1}} &= \sum_q \frac{\partial a_j^l}{\partial z_q^l} \frac{\partial z_q^l}{\partial a_k^{l-1}} \\
&= \sum_q \frac{\partial a_j^l}{\partial z_q^l} w_{qk}^l \\
g_{jk}^l &= A'(z_j^l) \odot w_{jk}^l
\end{align}

Going 1 more layer back:
\begin{align}
\frac{\partial a_j^l}{\partial a_k^{l-2}} &= \sum_q \left[ ( A'(z_j^l) \odot w_{jq}^l ) ( A'(z_q^{l-1}) \odot w_{qk}^{l-1} ) \right] \\
&= \sum_q g_{jq}^l g_{qk}^{l-1}
\end{align}

This relationship can be recurred to obtain:
\begin{equation}
J_{jk}^{l} \equiv \frac{\partial a_j^l}{\partial x_k} = \sum_{\alpha, \beta ... } g_{j\alpha}^l g_{a\beta}^{l-1} ... g_{\mu k}^{1}
\end{equation}

\section*{Network Hessian:}
The general \(l^{th}\) layer Hessian can be expressed as:
\begin{equation}
H_{jkm}^l \equiv \frac{\partial^2 a_j^l}{\partial x_m \partial x_k} = \frac{\partial J_{jk}^l}{\partial x_m}
\end{equation}

For constructing Physics Loss functions, often only the diagonal Hessian is needed. Let:
\begin{equation}
\sum_k \delta_{mk}H_{jkm}^l \equiv H_{jk}^l
\end{equation}
A further useful property of the full Hessian is its symmetry in the \(2^{nd}\) and \(3^{rd}\) indices (by property of partial derivatives):
\begin{equation}
H_{jkm} = H_{jmk}
\end{equation}

We can also note the useful base cases:
\begin{align}
J_{jk}^0 &= \delta_{jk} \\
H_{jkm}^0 &= 0
\end{align}

To derive the full Hessian we consider a recursive method and start from:
\begin{equation}
J_{jk}^l \equiv \sum_q g_{jq}^l J_{qk}^{l-1} = \sum_q \left[ A'(z_j^l) \odot w_{jq}^l J_{qk}^{l-1} \right]
\end{equation}

Considering the Hessian as the derivative of the Jacobian:
\begin{align}
H_{jkm}^l &= \frac{\partial}{\partial x_m} \sum_q \left[ A'(z_j^l) \odot w_{jq}^l J_{qk}^{l-1} \right] \\
&= \sum_q \left[ A''(z_j^l) \odot \frac{\partial z_j^l}{\partial x_m} \odot w_{jq}^l J_{qk}^{l-1} + A'(z_j^l) \odot w_{jq}^l \frac{\partial J_{qk}^{l-1}}{\partial x_m} \right]
\end{align}

We can recover several familiar terms from this expression, starting with
\begin{align}
\frac{\partial J_{qk}^{l-1}}{\partial x_m} &\equiv H_{qkm}^{l-1},
\end{align}
\indent and likewise, simplifying the first term:
\begin{align}
\frac{\partial z_j^l}{\partial x_m} 
&= \frac{\partial}{\partial x_m} \sum_p \left[ w_{jp}^l a_p^{l-1} + b_j^l \right]
= \sum_p w_{jp}^l \frac{\partial a_p^{l-1}}{\partial x_m}
= \sum_p w_{jp}^l J_{pm}^{l-1}
= \frac{J_{jm}^l}{A'(z_j^l)}
\end{align}

Putting these expressions together we yield a recursion relation for the full Hessian:
\begin{align}
H_{jkm}^l &= \frac{A''(z_j^l)}{A'(z_j^l)} \odot J_{jm}^l \sum_q w_{jq}^l J_{qk}^{l-1} + A'(z_j^l) \sum_q w_{jq}^l H_{qkm}^{l-1} \\
&= \frac{A''(z_j^l)}{\left[ A'(z_j^l) \right]^2} \odot J_{jm}^l \odot J_{jk}^l + A'(z_j^l) \sum_q w_{jq}^l H_{qkm}^{l-1}
\end{align}
Finally, we can contract over \(m \, \text{\&} \, k \) to obtain the diagonal Hessian of the \(l^{th}\) layer activations (with respect to the inputs \(x_k\):
\begin{align}
H_{jk}^l = \frac{A''(z_j^l)}{\left[ A'(z_j^l) \right]^2} \odot ( J_{jk}^l )^2+ A'(z_j^l) \sum_q w_{jq}^l H_{qk}^{l-1}
\end{align}

Although the above forms are probably the more succinct and interpretable, the divisions by \(A'\) prove problematic when activation functions have stationary points. Undoing some of the substitutions we can return to more computationally stable expressions:
\begin{align}
H_{jkm}^l &= A''(z_j^l) \left[ \sum_q w_{jq}^l J_{qm}^{l-1} \right] \left[ \sum_q w_{jq}^l J_{qk}^{l-1} \right] + A'(z_j^l) \sum_q w_{jq}^l H_{qkm}^{l-1} \\
H_{jk}^l &= A''(z_j^l) \left[ \sum_q w_{jq}^l J_{qk}^{l-1} \right]^2 + A'(z_j^l) \sum_q w_{jq}^l H_{qk}^{l-1}
\end{align}


\section*{Jacobian and Hessian Loss Derivatives:}
If we construct a Physics Loss using Jacobian and/or diagonal Hessian terms, we will need to propagate the according loss derivative through the network (standard backpropagation). This will require: \(\frac{\partial J_{jk}^L}{\partial a_m^L}\) and \(\frac{\partial H_{jk}^L}{\partial a_m^L}\). It turns out that both terms can be obtained quite easily using the chain rule:
\begin{align}
\frac{\partial J_{jk}^L}{\partial a_m^L} &= \sum_q \frac{\partial x_q}{\partial a_m^L} \frac{J_{jk}^L}{\partial x_q} \\
&= \sum_q \frac{H_{jkq}^L}{J_{mq}^L}
\end{align}
Similarly for the Hessian:
\begin{equation}
\frac{\partial H_{jk}^L}{\partial a_m^L} = \sum_q \frac{T_{jkq}^L}{J_{mq}^L}
\end{equation}
Where we have let \( T_{jkq}^L \equiv \frac{\partial H_{jk}^L}{\partial x_q} \) which turns out to be reasonably straightforward to derive recursively. Bearing in mind the issues of computational stability, we begin the derivation from Eqn. (22):
\begin{align}
T_{jkm}^l &= \frac{\partial}{\partial x_m} \Bigg[ A''(z_j^l) \left[ \sum_q w_{jq}^l J_{qk}^{l-1} \right]^2 + A'(z_j^l) \sum_q w_{jq}^l H_{qk}^{l-1} \Bigg] \\
&= A'''(z_j^l) \frac{\partial z_j^l}{\partial x_m} \left[ \sum_q w_{jq}^l J_{qk}^{l-1} \right]^2 + 2A''(z_j^l) \sum_q w_{jq}^l H_{qkm}^{l-1}  \\
&+ A''(z_j^l) \frac{\partial z_j^l}{\partial x_m} \sum_q w_{jq}^l H_{qk}^{l-1} + A'(z_j^l) \sum_q w_{jq}^l T_{qkm}^{l-1} \\
&= \left[ \sum_q w_{jq}^l J_{qm}^{l-1} \right] \Bigg[ A'''(z_j^l) \left[ \sum_q w_{jq}^l J_{qk}^{l-1} \right]^2 + A''(z_j^l) \sum_q w_{jq}^l H_{qk}^{l-1} \Bigg] \\
&+ 2A''(z_j^l) \sum_q w_{jq}^l H_{qkm}^{l-1} + A'(z_j^l) \sum_q w_{jq}^l T_{qkm}^{l-1}
\end{align}
Where the relationship in Eqn. (18) has been used to substitute the \(\frac{\partial z_j^l}{\partial x_m}\) terms.










\end{document}